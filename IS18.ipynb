{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated 3D Reconstruction from Satellite Images\n",
    "##  _SIAM IS18 MINITUTORIAL - 08/06/2018_\n",
    "### Gabriele Facciolo, Carlo de Franchis, and Enric Meinhardt-Llopis\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><img src=\"https://gfacciol.github.io/IS18/step1_150.png\"></th>\n",
    "    <th><img src=\"https://gfacciol.github.io/IS18/step2_150.png\"></th>    \n",
    "    <th><img src=\"https://gfacciol.github.io/IS18/step3_150.png\"></th>    \n",
    "    <th><img src=\"https://gfacciol.github.io/IS18/step4_150.png\"></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "\n",
    "This tutorial is a hands-on introduction to the manipulation of optical satellite images. The objective is to provide all the tools needed to process and exploit the images for 3D reconstruction. We will present the essential modeling elements needed for building a stereo pipeline for satellite images. This includes the specifics of satellite imaging such as pushbroom sensor modeling, coordinate systems, and localization functions. \n",
    "\n",
    "\n",
    "This notebook is divided in three sections.\n",
    "\n",
    "1. **Coordinate Systems and Geometric Modeling of Optical Satellites.** Introduces geographic coordinates,  and sensor models needed to manipulate satellite images. \n",
    "2. **Epipolar Rectification and Stereo Matching.** Introduces an approximated sensor model which is used to rectify pairs of satellite images and compute correspondences between them.\n",
    "3. **Triangulation and Digital Elevation Models.** Creates a point cloud by triangulating the correspondences then projects them on an UTM reference system.\n",
    "\n",
    "\n",
    "First we setup the tools needed for rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"color:blue;background:yellow\">Jupyter notebook usage: press <span style=\"color:black\">SHIFT+ENTER</span> to run one cell and go to the next one</span></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard modules used through the notebook \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tools specific for this tutorial\n",
    "# They are in the .py files accompaining this notebook \n",
    "import vistools      # display tools\n",
    "import utils         # IO tools\n",
    "import srtm4         # SRTM tools\n",
    "import rectification # rectification tools\n",
    "import stereo        # stereo tools\n",
    "import triangulation # triangulation tools\n",
    "from   vistools import printbf   # boldface print\n",
    "\n",
    "# Display and interface settings (just for the notebook interface)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "np.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Coordinate Systems and Geometric Modeling\n",
    "\n",
    "\n",
    "In this first section we'll learn:\n",
    "\n",
    "* about geodetic (longitude, latitude)  and projected (UTM) coordinates \n",
    "* to manipulate large satellite images \n",
    "* RPC camera model for localization and projection\n",
    "\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate systems\n",
    "\n",
    "Coordinate reference systems (CRS) provide a standardized way of describing geographic locations.\n",
    "Determining the shape of the earth is the first step in developing a CRS.\n",
    "A natural choice for describing points in 3d relative to the **ellipsoid**, is using [latitude, longitude, and altitude](https://en.wikipedia.org/wiki/World_Geodetic_System#A_new_World_Geodetic_System:_WGS_84). These are unprojected (or geographic) reference systems. \n",
    "Projected systems, on the other hand, are used for referencing locations on 2d\n",
    "representations of the Earth. \n",
    "\n",
    "\n",
    "\n",
    "<img style=\"float: right; width:200px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Latitude_and_longitude_graticule_on_an_ellipsoid.svg/200px-Latitude_and_longitude_graticule_on_an_ellipsoid.svg.png\"/>\n",
    "\n",
    "\n",
    "#### Geodetic Longitude, Latitude, and WGS84\n",
    "\n",
    "The [World Geodetic System (WGS84)](https://en.wikipedia.org/wiki/World_Geodetic_System#WGS84) is a standard for use in cartography, geodesy, navigation, GPS. It comprises a standard coordinate system for the Earth, a standard  reference ellipsoid to express altitude data, and a gravitational equipotential surface (the geoid) that defines the nominal sea level.  \n",
    "\n",
    "- [The geodetic latitude](https://en.wikipedia.org/wiki/Latitude)\n",
    "(usually denoted as φ) is the **angle between the equatorial plane** and a line that is **normal to the reference ellipsoid**.\n",
    "Note that the normal to the ellipsoid does not pass through the center, except at the equator and at the poles. \n",
    "- [The longitude](https://en.wikipedia.org/wiki/Longitude) of a point on Earth's surface is the angle east or west of a reference Greenwich meridian to another meridian that passes through that point. \n",
    "\n",
    "\n",
    "\n",
    "<table style=\"float: right;\" >\n",
    "  <tr>\n",
    "    <th>\n",
    "<img style=\"float: center;\"  width=\"150px\"  src=\"https://upload.wikimedia.org/wikipedia/commons/f/f4/Mercator_projection_SW.jpg\" />\n",
    "    </th>\n",
    "    <th>\n",
    "<img style=\"float: center;\"  width=\"200px\"  src=\"https://upload.wikimedia.org/wikipedia/commons/e/e2/Cylindrical_Projection_basics2.svg\" />\n",
    "    </th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Projections: Mercator and UTM\n",
    "\n",
    "Projections transform the elliptical earth into a flat surface.\n",
    "It is impossible to flatten a round object\n",
    "without distortion. This results in trade-offs between area,\n",
    "direction, shape, and distance. \n",
    "\n",
    "\n",
    "- [**The Mercator projection**](https://en.wikipedia.org/wiki/Mercator_projection) (used in Google maps) is a cylindrical map projection that is conformal so it preserves angles (which is usefull for navigation).\n",
    "The Mercator projection does not preserve areas, but **it is most accurate around the equator, where it is tangent to the globe**. \n",
    "\n",
    "\n",
    "\n",
    "<img style=\"float: right;\"  width=\"300px\"         src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Transverse_mercator_graticules.svg/800px-Transverse_mercator_graticules.svg.png\" />\n",
    "\n",
    "- [**The Universal Transverse Mercator (UTM)**](https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system) system is not a single map projection. The system instead divides the Earth into sixty **zones, each being a six-degree band of longitude**, and uses a secant transverse Mercator projection in each zone. \n",
    "Within an UTM zone the coordinates are expressed as easting and northing.\n",
    "The **easting** coordinate refers to the eastward-measured distance (in meters) from the central meridian of the UTM zone. While the **northing** coordinate refers to the distance to the equator. The northing of a point south of the equator is equal to 10000000m minus its distance from the equator (this way there are no negative coordinates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data available for this tutorial\n",
    "\n",
    "Since high-resolution WorldView-3 images are not in general freely downloadable (you have to buy them), a [sample set of publicly available images](http://www.jhuapl.edu/pubgeo/satellite-benchmark.html) is provided in a remote folder. The content of that folder can be listed with the `listFD` function of the `utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the tiff images available in the remote folder\n",
    "IARPAurl = 'http://avocat.ovh.hw.ipol.im:80/IARPA_data/cloud_optimized_geotif'\n",
    "myimages = utils.listFD(IARPAurl, 'TIF')\n",
    "\n",
    "# sort the images by acquisition date\n",
    "myimages = sorted(myimages, key=utils.acquisition_date)\n",
    "print('Found {} images'.format(len(myimages)))\n",
    "\n",
    "# select the two images to start working\n",
    "idx_a, idx_b = 0, 5\n",
    "print(\"Images Used:\")\n",
    "print(myimages[idx_a])\n",
    "print(myimages[idx_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images geographic footprints\n",
    "\n",
    "The longitude, latitude bounding box of a GeoTIFF image is described in its metadata. The `get_image_longlat_polygon` of the `utils` module can read it. Let's use it to display on a map the footprints of the selected images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates an interactive map and returns a map handle to interact with it.\n",
    "mymap = vistools.clickablemap(zoom=12)\n",
    "display(mymap)\n",
    "\n",
    "# display the footprint polygons of the satellite images \n",
    "for f in [idx_a, idx_b]:\n",
    "    \n",
    "    footprint = utils.get_image_longlat_polygon(myimages[f])\n",
    "    mymap.add_GeoJSON(footprint)\n",
    "\n",
    "# center the map on the center of the footprint\n",
    "mymap.center = np.mean(footprint['coordinates'][0][:4], axis=0).tolist()[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates of the area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the coordinates of the area of interest as a GeoJSON polygon\n",
    "# Buenos aires AOI\n",
    "aoi_buenos_aires = {'coordinates': [[[-58.585185, -34.490883],\n",
    "   [-58.585185, -34.48922],\n",
    "   [-58.583104, -34.48922],\n",
    "   [-58.583104, -34.490883],\n",
    "   [-58.585185, -34.490883]]],\n",
    " 'type': 'Polygon'}\n",
    "# add center field\n",
    "aoi_buenos_aires['center'] = np.mean(aoi_buenos_aires['coordinates'][0][:4], axis=0).tolist()\n",
    "    \n",
    "# add a polygon and center the map\n",
    "mymap.add_GeoJSON(aoi_buenos_aires)  # this draws the polygon described by aoi\n",
    "mymap.center = aoi_buenos_aires['center'][::-1]  # aoi_buenos_aires['coordinates'][0][0][::-1] \n",
    "mymap.zoom = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric modeling of optical satellites\n",
    "\n",
    "### The Rational Polynomial Camera Model\n",
    "\n",
    "Image vendors usually provide the orientation parameters of the cameras along with the images.\n",
    "To save their customers the tedious task of understanding and\n",
    "implementing each specific geometric camera model, they provide instead the *localization* and *projection* functions $L$ and $P$ associated to each image.\n",
    "These functions allow converting from image coordinates to coordinates\n",
    "on the globe and back. \n",
    "\n",
    "- The projection function $P:\\mathbb{R}^3\\to\\mathbb{R}^2$,\n",
    "$(\\lambda, \\theta, h) \\mapsto \\textbf{x}$ returns the image coordinates, in pixels, of a given 3-space\n",
    "point represented by its spheroidal coordinates in the World Geodetic\n",
    "System (WGS 84) identified by its\n",
    "longitude, latitude and\n",
    "altitude $h$ (in meters) above the reference ellipsoid. \n",
    "\n",
    "- The localization function $L:\\mathbb{R}^3\\to\\mathbb{R}^2$, $(\\textbf{x}, h) \\mapsto (\\lambda, \\theta)$ is its\n",
    "inverse with respect to the first two components. It takes a point $\\textbf{x}\n",
    "= (x, y)^\\top$ in the image domain together with an altitude $h$, and\n",
    "returns the geographic coordinates of the unique 3-space point\n",
    "$\\textbf{X} = (\\lambda, \\theta, h)$.\n",
    "\n",
    "<!-- <img src=\"fig/Latitude_and_Longitude_of_the_Earth.svg\" alt=\"Longitude and latitude\" style=\"width: 400px\"/>-->\n",
    "\n",
    "<img src=\"fig/rpc_illustration.svg\" alt=\"Projection and localization functions\" style=\"width: 300px; float:right\"/>\n",
    "\n",
    "***The *Rational Polynomial Coefficient* ($\\scriptsize{\\text{RPC}}$) camera model is an\n",
    "analytic description of the projection and localization functions*** [(Baltsavias & Stallmann'92)](http://dx.doi.org/10.3929/ethz-a-004336038), [(Tao & Hu'01)](http://eserv.asprs.org/PERS/2001journal/dec/2001_dec_1347-1357.pdf). Projection and\n",
    "localization functions are expressed as ratio of multivariate cubic\n",
    "polynomials. \n",
    "For example, the latitude component of the localization\n",
    "function for the image point $(x, y)$ at altitude $h$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = \\frac{\\sum_{i=1}^{20} C^{\\theta, \\tiny{\\text{NUM}}}_i \\rho_i(x, y, h)}{\\sum_{i=1}^{20} C^{\\theta, \\tiny{\\text{DEN}}}_i \\rho_i(x, y, h)}\n",
    "\\end{equation}\n",
    "\n",
    "where $C^{\\theta, \\tiny{\\text{NUM}}}_i$ (resp.\n",
    "$C^{\\theta, \\tiny{\\text{DEN}}}_i$) is the $i^{\\text{th}}$ coefficient of the\n",
    "numerator (resp. denominator) polynomial and $\\rho_{i}$ produces the\n",
    "$i^{\\text{th}}$ factor of the three variables cubic polynomial. \n",
    "A cubic polynomial in three variables has 20 coefficients, thus each\n",
    "component of the localization and projection functions requires 40\n",
    "coefficients. Ten additional parameters specify the scale and\n",
    "offset for the five variables $x, y, \\lambda, \\theta$ and $h$. \n",
    "\n",
    "\n",
    "$\\scriptsize{\\text{RPC}}$ localization and projection functions\n",
    "are not exact inverses of each other. The errors due to\n",
    "concatenating the projection and inverse functions are negligible, being\n",
    "of the order of $10^{-7}$ degrees in longitude and latitude, i.e. about 1 cm\n",
    "on the ground or $\\frac{1}{100}$ of pixel in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images RPC coefficients\n",
    "\n",
    "The 90 coefficients (20 \\* 2 \\* 2 + 10) of the RPC projection function associated to each image are stored in the image GeoTIFF header. They can be read with the `rpc_from_geotiff` function of the `utils` module. This function returns an instance of the class `rpc_model.RPCModel` which contains the RPC coefficients and a `projection` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrpcs = [utils.rpc_from_geotiff(x) for x in myimages]\n",
    "rpc = myrpcs[idx_a]\n",
    "print(rpc)\n",
    "\n",
    "# let's try the projection method\n",
    "lon, lat = aoi_buenos_aires['center']\n",
    "x, y = rpc.projection(lon, lat, 0)\n",
    "print(\"\\n\\nThe pixel coordinates (in image idx_a) of our AOI center\\n\"\n",
    "      \"(lon=%.4f, lat=%.4f) at altitude 0 are: (%f, %f)\" % (lon, lat, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:red\">Exercise 1** \n",
    "<span style=\"color:red\">Complete the implementation of the `crop_aoi` function below. This function crops an area of interest (AOI) defined with geographic coordinates in a GeoTIFF image using its RPC functions.</span>\n",
    "\n",
    "It takes as input arguments:\n",
    "* `geotiff`: path to the input GeoTIFF image file\n",
    "* `aoi`: GeoJSON polygon\n",
    "* `z`: ground altitude with respect to the WGS84 ellipsoid\n",
    "\n",
    "It returns:\n",
    "* `crop`: a numpy array containing the image crop\n",
    "* `x, y`: integer pixel coordinates of the top left corner of the crop in the input image\n",
    "\n",
    "To complete this function you need to use:\n",
    "* `utils.rpc_from_geotiff` to read the RPC coefficients and get an `rpc_model.RPCModel` object\n",
    "* the `projection` method of the `rpc_model.RPCModel` object\n",
    "* `utils.bounding_box2D` to compute a horizontal/vertical rectangular bounding box\n",
    "* `utils.rio_open` to open the image with the `rasterio` package\n",
    "* the `read(window=())` method of a `rasterio` object to read a window of the image\n",
    "\n",
    "\n",
    "The `projection` function needs an altitude coordinate `z`, which **is not** contained in the `aoi` GeoJSON polygon. We may **assume that `z` is zero**, or alternatively **get `z` from an external Digital Elevation Model (DEM) such as SRTM**. The SRTM altitude at a given `longitude, latitude` obtained using the `srtm4` module.\n",
    "\n",
    "\n",
    "**The code below calls your `crop_aoi` function** to crop the area selected in the map from image idx_a and displays the crop. The altitude is evaluated using the the `srtm4` function. Vefify that the image corresponds to the area selected  above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_aoi(geotiff, aoi, z=0):\n",
    "    \"\"\"\n",
    "    Crop a geographic AOI in a georeferenced image using its RPC functions.\n",
    "\n",
    "    Args:\n",
    "        geotiff (string): path or url to the input GeoTIFF image file\n",
    "        aoi (geojson.Polygon): GeoJSON polygon representing the AOI\n",
    "        z (float): base altitude with respect to WGS84 ellipsoid (0 by default)\n",
    "\n",
    "    Return:\n",
    "        bbox: x, y, w, h image coordinates of the crop. x, y are the\n",
    "            coordinates of the top-left corner, while w, h are the dimensions\n",
    "            of the crop.\n",
    "    \"\"\"\n",
    "    # extract the rpc from the geotiff file\n",
    "    rpc = utils.rpc_from_geotiff(geotiff)\n",
    "    # put the aoi corners in an array\n",
    "    Clonlat = np.array(aoi['coordinates'][0])             # 4 coordinates (lon,lat)\n",
    "\n",
    "    # project Clonlat into the image\n",
    "    # INSERT A LINE FOR COMPUTING THE FOUR x,y IMAGE COORDINATES  \n",
    "    # STARTING FROM THE longitude and latitude in: Clonlat[:,0] Clonlat[:,1]    \n",
    "    #x, y = rpc.projection(Clonlat[:,0], Clonlat[:,1], z)\n",
    "\n",
    "    # convert the list into array\n",
    "    pts = np.array([x, y])                          # all coordinates (pixels)\n",
    "    # compute the bounding box in pixel coordinates\n",
    "    bbox = utils.bounding_box2D(pts.transpose())\n",
    "    x0, y0, w, h = np.round(bbox).astype(int)\n",
    "    # crop the computed bbox from the large GeoTIFF image\n",
    "    with utils.rio_open(geotiff, 'r') as src:\n",
    "        crop = src.read(window=((y0, y0 + h), (x0, x0 + w)))\n",
    "    return crop, x0, y0\n",
    "\n",
    "\n",
    "\n",
    "# get the altitude of the center of the AOI\n",
    "lon, lat  = aoi_buenos_aires['center']\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "# crop the selected AOI in image number 10\n",
    "crop, x, y = crop_aoi(myimages[idx_a], aoi_buenos_aires, z)\n",
    "\n",
    "# display the crop\n",
    "vistools.display_imshow(utils.simple_equalization_8bit(crop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization function\n",
    "\n",
    "The _localization_ function is the inverse of the _projection_ function with respect to the image coordinates. It takes as input a triplet `x, y, z`, where `x` and `y` are pixel coordinates and `z` is the altitude of the corresponding 3D point above the WGS84 ellipsoid. It returns the longitude `lon` and latitude `lat` of the 3D point.\n",
    "\n",
    "The code below projects a 3D point on the image, localizes this image point on the ground, and then **computes the distance to the original point**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  numpy.linalg import norm as l2norm\n",
    "# get the altitude of the center of the AOI\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "# project a 3D point on the image\n",
    "x, y = rpc.projection(lon, lat, z)\n",
    "\n",
    "# localize this image point on the ground\n",
    "new_lon, new_lat = rpc.localization(x, y, z)\n",
    "\n",
    "# compute the distance to the original point\n",
    "print( \"Error of the inverse: {} pixels\".format( l2norm([new_lon - lon, new_lat - lat]) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Epipolar Rectification and Stereo Matching\n",
    "\n",
    "In this section we will learn to compute correspondences between a pair of images.\n",
    "These correspondences will be used in the next section for computing 3D models.\n",
    "\n",
    "The basic scheme is the following:\n",
    "1. extract, rotate, rescale, and shear a portion of each image so that epipolar lines are horizontal and coincident \n",
    "2. apply a standard stereo-matching algorithm such as SGM using a robust matching cost\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epipolar curves\n",
    "\n",
    "The following illustration displays the epipolar curve corresponding to a point in the first image. \n",
    "The function samples the epipolar curve of a pair of images by composing the _localization_ function of the first image with the _projection_ function of the second image.\n",
    "\n",
    "**Note that the resulting line is practically a straight line!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectification.trace_epipolar_curve(myimages[37], myimages[38], aoi_buenos_aires, x0=220, y0=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine approximation of the camera model\n",
    "\n",
    "Let $P: \\mathbb{R}^3\\longrightarrow \\mathbb{R}^2$ be the _projection_ function. The first order Taylor approximation of $P$ around point $X_0$ is $P(X) = P(X_0) + \\nabla P(X_0)(X - X_0)$, which can be rewritten as\n",
    "\n",
    "$$\n",
    "P(X) = \\nabla P(X_0)X + T\n",
    "$$\n",
    "\n",
    "with $\\nabla P(X_0)$ the jacobian matrix of size (2, 3) and $T = P(X_0) - \\nabla P(X_0) X_0$ a vector of size 2. This can be rewritten as a linear operation by using homogeneous coordinates: with $X = (\\lambda, \\varphi, h, 1)$ the previous formula becomes $P(X) = AX$, where the (3, 4) matrix $A$ is the _affine approximation_ of the RPC _projection_ function $P$ at point $X_0$. \n",
    "\n",
    "The code below calls the `rpc_affine_approximation` function to compute the affine camera matrix approximating the RPC _projection_ function around the center $X_0$ of the area selected in the map. Then it evaluates the approximation error away from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the altitude of the center of the AOI\n",
    "lon, lat = aoi_buenos_aires['center']\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "# compute the affine projection matrix \n",
    "A = rectification.rpc_affine_approximation(rpc, (lon, lat, z))   # affine projection matrix for first image\n",
    "\n",
    "# approximation error at the center \n",
    "err = l2norm( (A @ [lon, lat, z, 1])[:2] - np.array(rpc.projection(lon, lat, z)) )\n",
    "print(\"Error at the center: {} pixels\".format(err))\n",
    "\n",
    "# compute the projection in the image \n",
    "x, y = rpc.projection(lon, lat, z)\n",
    "lon1, lat1 = rpc.localization(x + 500, y + 500, z)\n",
    "\n",
    "# approximation error at center +500,+500\n",
    "err = l2norm( (A @ [lon1, lat1, z, 1])[:2] - np.array(rpc.projection(lon1, lat1, z)) )\n",
    "print(\"Error away from the center: {} pixels\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine rectification\n",
    "\n",
    "\n",
    "The operation of resampling a pair of images such that the epipolar lines become horizontal and aligned is called _stereo rectification_ or _epipolar resampling_. \n",
    "\n",
    "Using the affine camera approximation, this rectification reduces to computing two planar affine transformations that map the epipolar lines to a set of matching horizontal lines.\n",
    "\n",
    "The code below defines the function `rectify_aoi` that computes two rectifying affine transforms for the two images. The affine transforms are composed of a rotation and a zoom (to ensure aligned horizontal epipolar lines) plus an extra affine term to ensure that the ground (horizontal plane at altitude `z`) is registered. An extra translation ensures that the rectified images contain the whole area of interest and nothing more.\n",
    "\n",
    "The function `rectify_aoi` then resamples the two images according to the rectifying affine transforms, and computes sift keypoint matches to estimate the disparity range. This will be needed as an input for the stereo-matching algorithm in the next section.\n",
    "\n",
    "The rectified images are displayed in a gallery. Flip between the images to see how the buildings move!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify_aoi(file1, file2, aoi, z=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file1, file2 (strings): file paths or urls of two satellite images\n",
    "        aoi (geojson.Polygon): area of interest\n",
    "        z (float, optional): base altitude with respect to WGS84 ellipsoid. If\n",
    "            None, z is retrieved from srtm.\n",
    "\n",
    "    Returns:\n",
    "        rect1, rect2: numpy arrays with the images\n",
    "        S1, S2: transformation matrices from the coordinate system of the original images\n",
    "        disp_min, disp_max: horizontal disparity range\n",
    "        P1, P2: affine rpc approximations of the two images computed during the rectification\n",
    "    \"\"\"\n",
    "    # read the RPC coefficients\n",
    "    rpc1 = utils.rpc_from_geotiff(file1)\n",
    "    rpc2 = utils.rpc_from_geotiff(file2)\n",
    "\n",
    "    # get the altitude of the center of the AOI\n",
    "    if z is None:\n",
    "        lon, lat = np.mean(aoi['coordinates'][0][:4], axis=0)\n",
    "        z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "    # compute rectifying affine transforms\n",
    "    S1, S2, w, h, P1, P2 = rectification.rectifying_affine_transforms(rpc1, rpc2, aoi, z=z)\n",
    "\n",
    "    # compute sift keypoint matches\n",
    "    q1, q2 = rectification.sift_roi(file1, file2, aoi, z)\n",
    "\n",
    "    # transform the matches to the domain of the rectified images\n",
    "    q1 = utils.points_apply_homography(S1, q1)\n",
    "    q2 = utils.points_apply_homography(S2, q2)\n",
    "\n",
    "    # CODE HERE: insert a few lines to correct the vertical shift\n",
    "    y_shift = 0\n",
    "    #y_shift = np.median(q2 - q1, axis=0)[1]\n",
    "    S2 = rectification.matrix_translation(-0, -y_shift) @ S2\n",
    "\n",
    "    # rectify the crops\n",
    "    rect1 = rectification.affine_crop(file1, S1, w, h)\n",
    "    rect2 = rectification.affine_crop(file2, S2, w, h)\n",
    "\n",
    "    # disparity range bounds\n",
    "    kpts_disps = (q2 - q1)[:, 0]\n",
    "    disp_min = np.percentile(kpts_disps, 2)\n",
    "    disp_max = np.percentile(kpts_disps, 100 - 2)\n",
    "\n",
    "    return rect1, rect2, S1, S2, disp_min, disp_max, P1, P2\n",
    "\n",
    "\n",
    "rect1, rect2, S1, S2, disp_min, disp_max, P1, P2 = rectify_aoi(myimages[idx_a], \n",
    "                                                               myimages[idx_b], \n",
    "                                                               aoi_buenos_aires, z=14)\n",
    "\n",
    "# display the rectified crops\n",
    "vistools.display_gallery([utils.simple_equalization_8bit(rect1),\n",
    "                          utils.simple_equalization_8bit(rect2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rectification above has failed!    The images are not \"vertically aligned\"\n",
    "\n",
    "### **<span style=\"color:red\">Exercise 2</span>** \n",
    "<span style=\"color:red\">Improve the implementation of the `rectify_aoi` function above so that it corrects the vertical alignement observed in this rectified pair. Use the SIFT keypoint matches to estimate the required vertical correction.</span>\n",
    "\n",
    "After correcting the rectification you should see only horizontal displacements!\n",
    "The relative pointing error is particularly visible in image pairs (0, 5) and (0, 11). In other image pairs, such as (27, 28), the error is very small and almost invisible.\n",
    "\n",
    "**The corrected stereo-rectified pairs of image crops will be the input for the stereo matching algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stereo matching \n",
    "\n",
    "Stereo matching computes the correspondences between a pair of rectified images. \n",
    "We use the [Semi Global Matching (SGM) algorithm (Hirschmüller'06)](https://ieeexplore.ieee.org/document/1467526/). SGM is an approximate energy minimization algorithm based on Dynamic Programming. \n",
    "\n",
    "Two critical components of the matching algorithms are:\n",
    "* **Choice of matching cost.** The usual squared differences cost (sd) is not robust to illumination changes or artifacts often present in satellite images. For this reason the Hamming distance between [Census Transforms (Zabih & Woodfill'94)](https://link.springer.com/chapter/10.1007/BFb0028345) is preferred. \n",
    "\n",
    "* **Disparity post-processing.** To remove the spurious matches the disparity map must be filtered. First by applying a left-right consistency test, then removing speckes (small connected disparity components that have a disparity inconsistent with neighborhood).\n",
    "\n",
    "The function \n",
    "```\n",
    "compute_disparity_map(im1, im2, dmin, dmax, cost='census', lam=10)\n",
    "```\n",
    "computes disparity maps from two rectified images (`im1`, `im2`) using SGM,\n",
    "cost selects the matching cots (sd or census), and the  result is filtered for mismatches using left-right and speckle filters.\n",
    "    \n",
    "The code below calls the  `stereo.compute_disparity_map` function and compares the results obtained with `sd` and `census` costs with and without filtering.  \n",
    "\n",
    "**From now on we use a different image pair (idx_a, idx_b) as it yields more striking results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### select a new pair of images (but the same aoi)\n",
    "idx_a=37\n",
    "idx_b=38\n",
    "aoi = aoi_buenos_aires\n",
    "\n",
    "# crop and rectigy the images\n",
    "rect1, rect2, S1, S2, dmin, dmax, PA, PB = rectification.rectify_aoi(myimages[idx_a], \n",
    "                                                                     myimages[idx_b], \n",
    "                                                                     aoi)\n",
    "# add some margin to the estimated disparity range\n",
    "dmin, dmax = dmin-20, dmax+20\n",
    "\n",
    "\n",
    "# EXTRA: set True if you want to try with a standard stereo pair\n",
    "if False:  \n",
    "    dmin, dmax = -60,0\n",
    "    rect1=utils.readGTIFF('data/im2.png')\n",
    "    rect2=utils.readGTIFF('data/im6.png')\n",
    "\n",
    "\n",
    "# compute left and right disparity maps comparing SD and CENSUS  \n",
    "print('Disparity range: [%d, %d]'%(dmin,dmax))\n",
    "lambdaval=10\n",
    "LRSsd, dLsd, _ =  stereo.compute_disparity_map(rect1,rect2,dmin,dmax,cost='sd',  lam=lambdaval*10)\n",
    "LRS  , dL  , _ =  stereo.compute_disparity_map(rect1,rect2,dmin,dmax,cost='census', lam=lambdaval)\n",
    "\n",
    "# compare with sd and results without filtering results\n",
    "print('Comparison with sd cost and results without filtering results')\n",
    "vistools.display_gallery([utils.simple_equalization_8bit(LRS),\n",
    "                          utils.simple_equalization_8bit(LRSsd),\n",
    "                          utils.simple_equalization_8bit(dL),\n",
    "                          utils.simple_equalization_8bit(dLsd),\n",
    "                          utils.simple_equalization_8bit(rect1),\n",
    "                          utils.simple_equalization_8bit(rect2)\n",
    "                         ], \n",
    "                         ['census filtered', 'sd filtered','census', \n",
    "                          'sd','ref','sec'])\n",
    "\n",
    "# display the main result\n",
    "vistools.display_imshow(LRS, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.  Triangulation and Digital Elevation Models\n",
    "\n",
    "\n",
    "The extraction of 3D points from image correspondences is called *triangulation* (because the position of a point is found by trigonometry) or *intersection* (because it corresponds to the intersection of two light rays in space).  \n",
    "The goal of this section is to produce 3D a point cloud from two satellite images, and then project it on a geographic grid to produce a 2.5D model.\n",
    "In the context of geographic imaging, these 2.5D models are called *digital elevation model* (DEM).\n",
    "\n",
    "The plan is the following\n",
    "\n",
    "1. triangulate a single 3D point from one correspondence between two images\n",
    "2. triangulate a dense set of 3D points from two images\n",
    "3. project a 3D point cloud into a DEM\n",
    "\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangulation of a single point\n",
    "\n",
    "A pixel **p** in a satellite image *A* defines a line in space by means of the localization function $h\\mapsto L_A(\\mathbf{p},h)$.  This line is parametrized by the height *h*, and it is the set of all points in space that are projected into the pixel **p**:\n",
    "\n",
    "$$P_A(L_A(\\mathbf{p}),h),h)=\\mathbf{p} \\qquad \\forall h\\in\\mathbf{R}$$\n",
    "\n",
    "Now, when a point $\\mathbf{x}=(x,y,h)$ in space is projected into pixels **p**, **q** on images *A*,*B*, we will have the relations\n",
    "\n",
    "$$\\begin{cases}\n",
    "P_A(\\mathbf{x})=\\mathbf{p} \\\\\n",
    "P_B(\\mathbf{x})=\\mathbf{q} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Since **p** and **q** are pixel coordinates in the image domains, this is a system of four equations.  We can use this system of equations to find the 3D point **x** from the correspondence $\\mathbf{p}\\sim\\mathbf{q}$ by solving this system.  Notice that the system is over-determined, so in practice it will not have an exact solution and we may have to find a \"solution\" that has minimal error in some sense (e.g., least-squares).\n",
    "\n",
    "Another way to express the same relationship is via the localization functions:\n",
    "\n",
    "$$L_A(\\mathbf{p},h)=L_B(\\mathbf{p},h)$$\n",
    "\n",
    "Now this is a system of two equations and a single unknown $h$.  This system can be interpreted as the intersection of two lines in 3D space.\n",
    "\n",
    "In practice, the projection and localization functions are approximated using affine maps, thus all the systems above are linear overdetermined and can be solved readily using the Moore-Penrose pseudo-inverse (or, equivalently, least squares).  This algorithm is implemented in the function ``triangulation_affine`` on file ``triangulation.py``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we start by triangulating an artificial point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a point in the center of the region of interest\n",
    "Ra = myrpcs[idx_a]\n",
    "Rb = myrpcs[idx_b]\n",
    "x = [Ra.lon_offset, Ra.lat_offset, Ra.alt_offset]\n",
    "print(\"x = %s\"%(x))\n",
    "\n",
    "# project the point x into each image\n",
    "p = Ra.projection(*x)\n",
    "q = Rb.projection(*x)\n",
    "print(\"p = %s\\nq = %s\"%(p, q))\n",
    "\n",
    "# extract the affine approximations of each projection function\n",
    "Pa = rectification.rpc_affine_approximation(Ra, x)\n",
    "Pb = rectification.rpc_affine_approximation(Rb, x)\n",
    "\n",
    "# triangulate the correspondence (p,q)\n",
    "lon, lat, alt, err = triangulation.triangulation_affine(Pa, Pb, p[0], p[1], q[0], q[1])\n",
    "print(\"lon, lat, alt, err = %s, %s, %s, %s\"%(lon, lat, alt, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the point **x** is recovered exactly and the error (given in meters) is essentially zero.\n",
    "\n",
    "Now, we select the same point, by hand, in two different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a crop of each image, and SAVE THE CROP OFFSETS\n",
    "crop_a, offx_a, offy_a = crop_aoi(myimages[idx_a], aoi_buenos_aires, x[2])\n",
    "crop_b, offx_b, offy_b = crop_aoi(myimages[idx_b], aoi_buenos_aires, x[2])\n",
    "print(\"x0_a, y0_a = %s, %s\"%(offx_a, offy_a))\n",
    "print(\"x0_b, y0_b = %s, %s\"%(offx_b, offy_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates at the top of the tower, chosen by visual inspection of the images below\n",
    "p = [179, 274]\n",
    "q = [188, 296]\n",
    "\n",
    "# plot each image with the selected point as a red dot\n",
    "_,f = plt.subplots(1, 2, figsize=(13,10))\n",
    "f[0].imshow(np.log(crop_a.squeeze()), cmap=\"gray\")\n",
    "f[1].imshow(np.log(crop_b.squeeze()), cmap=\"gray\")\n",
    "f[0].plot(*p, \"ro\")\n",
    "f[1].plot(*q, \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a base point for affine approximations\n",
    "base_lon, base_lat = aoi_buenos_aires[\"center\"]\n",
    "base_z = srtm4.srtm4(base_lon,base_lat)\n",
    "base_x = [base_lon, base_lat, base_z]\n",
    "\n",
    "# extract the affine approximations of each projection function\n",
    "Pa = rectification.rpc_affine_approximation(myrpcs[idx_a], base_x)\n",
    "Pb = rectification.rpc_affine_approximation(myrpcs[idx_b], base_x)\n",
    "\n",
    "# triangulate the top of the tower (notice that the OFFSETS of each point are corrected)\n",
    "triangulation.triangulation_affine(Pa, Pb, p[0] + offx_a, p[1] + offy_a, q[0] + offx_b, q[1] + offy_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the height of the tower is 52 meters above the Earth ellipsoid.  Notice that to obtain a meaningful result, the offset of the crop has to be corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangulation of many points\n",
    "\n",
    "In practice, instead of finding the correspondences by hand we can use a stereo correlator on the rectified images.  In that case, the disparities have to be converted back to coordinates in the original image domain, by applying the inverse of the rectification map.  This is what the function ``triangulate_disparities`` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_disparities(dmap, rpc1, rpc2, S1, S2, PA, PB,):\n",
    "    \"\"\"\n",
    "    Triangulate a disparity map\n",
    "    \n",
    "    Arguments:\n",
    "        dmap : a disparity map between two rectified images\n",
    "        rpc1, rpc2 : calibration data of each image\n",
    "        S1, S2 : rectifying affine maps (from the domain of the original, full-size images)\n",
    "        PA, PB : the affine approximations of rpc1 and rpc2 (not always used)\n",
    "        \n",
    "    Return:\n",
    "        xyz : a matrix of size Nx3 (where N is the number of finite disparites in dmap)\n",
    "              this matrix contains the coordinates of the 3d points \n",
    "              in \"lon,lat,h\" or \"easting,northing,h\"\n",
    "    \"\"\"\n",
    "    from utils import utm_from_lonlat\n",
    "    \n",
    "    # 1. unroll all the valid (finite) disparities of dmap into a vector\n",
    "    m = np.isfinite(dmap.flatten())\n",
    "    x = np.argwhere(np.isfinite(dmap))[:,1]    # attention to order of the indices\n",
    "    y = np.argwhere(np.isfinite(dmap))[:,0]\n",
    "    d = dmap.flatten()[m]\n",
    "    \n",
    "    # 2. for all disparities\n",
    "    # 2.1. produce a pair of points in the original image domain by composing with S1 and S2\n",
    "    p = np.linalg.inv(S1) @ np.vstack( (x+0, y, np.ones(len(d))) )\n",
    "    q = np.linalg.inv(S2) @ np.vstack( (x+d, y, np.ones(len(d))) )\n",
    "    \n",
    "    # 2.2. triangulate the pair of image points to find a 3D point (in UTM coordinates)\n",
    "    lon, lat, h, err = triangulation.triangulation_affine(PA, PB, p[0,:], p[1,:], q[0,:], q[1,:])\n",
    "    \n",
    "    # 2.3. append points to the output vector\n",
    "    \n",
    "    # \"a meter is one tenth-million of the distance from the North Pole to the Equator\"\n",
    "    # cf. Lagrange, Laplace, Monge, Condorcet\n",
    "    factor = 1 # 1e7 / 90.0\n",
    "    xyz = np.vstack((lon*factor, lat*factor, h)).T\n",
    "    \n",
    "    #east, north = utm_from_lonlat(lon, lat)\n",
    "    #xyz = np.vstack((east, north, h)).T   \n",
    "    \n",
    "    return xyz\n",
    "\n",
    "\n",
    "xyz = triangulate_disparities(LRS, myrpcs[idx_a], myrpcs[idx_b], S1, S2, PA, PB)\n",
    "xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the point cloud\n",
    "display(vistools.display_cloud(xyz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This point cloud is all wrong!\n",
    "### The point cloud must be represented using cartesian coordinates (each coordinate using the same units)\n",
    "\n",
    "### **<span style=\"color:red\">Exercise 3</span>** \n",
    "<span style=\"color:red\">Modify the `triangulate_disparities` function to return points with coordinates in a cartesian system such as UTM. Use the function `utils.utm_from_lonlat`, wich can process vectors of longitudes (lon) latitudes (lat):\n",
    "</span>\n",
    "\n",
    "      east, north = utils.utm_from_lonlat(lon, lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digital elevation model projection\n",
    "\n",
    "The following call projects the point cloud represented in UTM coordinates into an grid to produce a DEM. The algorithm averages all the points that fall into each square of the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax, nmin, nmax = utils.utm_bounding_box_from_lonlat_aoi(aoi_buenos_aires)\n",
    "dem = triangulation.project_cloud_into_utm_grid(xyz, emin, emax, nmin, nmax, resolution=0.5)\n",
    "vistools.display_imshow(dem,  cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Section.  Complete Satellite Stereo Pipeline\n",
    "\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vistools      # display tools\n",
    "import utils         # IO tools\n",
    "import rectification # rectification tools\n",
    "import stereo        # stereo tools\n",
    "import triangulation # triangulation tools\n",
    "%matplotlib inline\n",
    "\n",
    "# list images and their rpcs\n",
    "IARPAurl = 'http://menthe.ovh.hw.ipol.im:80/IARPA_data/cloud_optimized_geotif'\n",
    "myimages = sorted(utils.listFD(IARPAurl, 'TIF'), key=utils.acquisition_date)\n",
    "myrpcs = [ utils.rpc_from_geotiff(x) for x in myimages ]\n",
    "print('Found {} images'.format(len(myimages)))\n",
    "\n",
    "# select an AOI\n",
    "aoi = {'coordinates': [[[-58.585185, -34.490883],\n",
    "   [-58.585185, -34.48922], [-58.583104, -34.48922],\n",
    "   [-58.583104, -34.490883],[-58.585185, -34.490883]]],\n",
    "   'type': 'Polygon'}\n",
    "\n",
    "# select an image pair\n",
    "idx_a, idx_b = 38, 39\n",
    "\n",
    "# run the whole pipeline\n",
    "rect1, rect2, S1, S2, dmin, dmax, PA, PB = rectification.rectify_aoi(myimages[idx_a], myimages[idx_b], aoi)\n",
    "LRS, _, _ = stereo.compute_disparity_map(rect1, rect2, dmin-20, dmax+20 , cost='census')\n",
    "xyz = triangulation.triangulate_disparities(LRS, myrpcs[idx_a], myrpcs[idx_b], S1, S2, PA, PB)\n",
    "emin, emax, nmin, nmax = utils.utm_bounding_box_from_lonlat_aoi(aoi)\n",
    "dem2 = triangulation.project_cloud_into_utm_grid(xyz, emin, emax, nmin, nmax, resolution=0.5)\n",
    "\n",
    "# display the input, the intermediate results and the output\n",
    "a, _, _ = utils.crop_aoi(myimages[idx_a], aoi)\n",
    "b, _, _ = utils.crop_aoi(myimages[idx_b], aoi)\n",
    "vistools.display_gallery([a/8,b/8])          # show the original images\n",
    "vistools.display_gallery([rect1/8,rect2/8])  # show the rectified images\n",
    "vistools.display_imshow(LRS, cmap='jet')     # show the disparity map\n",
    "display(vistools.display_cloud(xyz))    # show the point cloud\n",
    "vistools.display_imshow(dem2, cmap='jet')    # show the DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "[//]: # (© 2018 Gabriele Facciolo, Carlo de Franchis, and Enric Meinhardt-Llopis)\n",
    "[//]: # (<div style=\"text-align:center; font-size:75%;\"> Copyright © 2018 Gabriele Facciolo, Carlo de Franchis, and Enric Meinhardt-Llopis. All rights reserved.</div> )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
